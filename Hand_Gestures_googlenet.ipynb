{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hand Gestures-googlenet.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOAgHjGKGGGcPZFaDWo/xnT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreysingla11/3D-project/blob/master/Hand_Gestures_googlenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MrloZrfGjZ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from torchvision import transforms, datasets, models\n",
        "import torch\n",
        "from torch import optim, cuda\n",
        "from torch.utils.data import DataLoader, sampler\n",
        "import torch.nn as nn\n",
        "\n",
        "# Data science tools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Image manipulations\n",
        "from PIL import Image\n",
        "# Useful for examining network\n",
        "from torchsummary import summary\n",
        "# Timing utility\n",
        "from timeit import default_timer as timer\n",
        "\n",
        "# Visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2WvvvN7Gq8_",
        "colab_type": "code",
        "outputId": "229ab85d-a3a3-4a52-b110-658c35763d45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6DrJvqsTG8x0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = '/content/gdrive/My Drive/Handgestures/'\n",
        "\n",
        "train_dir = data_dir + 'train/'\n",
        "valid_dir = data_dir + 'valid/'\n",
        "test_dir = data_dir + 'test/'\n",
        "\n",
        "batch_size = 64\n",
        "save_file_name = 'densenet131-handgestures.pt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "panBbkOyHDfX",
        "colab_type": "code",
        "outputId": "195dc31c-d886-45cd-95e8-ca7d1fede23a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_on_gpu = cuda.is_available()\n",
        "print('Train on gpu :',train_on_gpu)\n",
        "multi_gpu = False"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on gpu : True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA3csOb1HFyd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(image):\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ32sNCmHLqC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Image transformations\n",
        "image_transforms = {\n",
        "    # Train uses data augmentation\n",
        "    'train':\n",
        "    transforms.Compose([\n",
        "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
        "        transforms.RandomRotation(degrees=15),\n",
        "        transforms.ColorJitter(),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.CenterCrop(size=224),  # Image net standards\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                             [0.229, 0.224, 0.225])  # Imagenet standards\n",
        "    ]),\n",
        "    # Validation does not use augmentation\n",
        "    'val':\n",
        "    transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "    # Test does not use augmentation\n",
        "    'test':\n",
        "    transforms.Compose([\n",
        "        transforms.Resize(size=256),\n",
        "        transforms.CenterCrop(size=224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ]),\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3Hq2coCHOGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow_tensor(image,ax=None,title=None):\n",
        "    if ax is None:\n",
        "        fig,ax = plt.subplots()\n",
        "    image = image.numpy().transpose((1,2,0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    image = std * image + mean\n",
        "    image = np.clip(image,0,1)\n",
        "    ax.imshow(image)\n",
        "    plt.axis('off')\n",
        "    return ax,image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU0WEAJAHQmw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Datasets from each folder\n",
        "data = {\n",
        "    'train':\n",
        "    datasets.ImageFolder(root=train_dir, transform=image_transforms['train']),\n",
        "    'val':\n",
        "    datasets.ImageFolder(root=valid_dir, transform=image_transforms['val']),\n",
        "    'test':\n",
        "    datasets.ImageFolder(root=test_dir, transform=image_transforms['test'])\n",
        "}\n",
        "\n",
        "# Dataloader iterators\n",
        "dataloaders = {\n",
        "    'train': DataLoader(data['train'], batch_size=batch_size, shuffle=True),\n",
        "    'val': DataLoader(data['val'], batch_size=batch_size, shuffle=True),\n",
        "    'test': DataLoader(data['test'], batch_size=batch_size, shuffle=True)\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs612jAvHSwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model,\n",
        "          criterion,\n",
        "          optimizer,\n",
        "          train_loader,\n",
        "          valid_loader,\n",
        "          save_file_name,\n",
        "          max_epochs_stop=3,\n",
        "          n_epochs=20):\n",
        "         \n",
        "    \"\"\"Train a PyTorch Model\n",
        "\n",
        "    Params\n",
        "    --------\n",
        "        model (PyTorch model): cnn to train\n",
        "        criterion (PyTorch loss): objective to minimize\n",
        "        optimizer (PyTorch optimizier): optimizer to compute gradients of model parameters\n",
        "        train_loader (PyTorch dataloader): training dataloader to iterate through\n",
        "        valid_loader (PyTorch dataloader): validation dataloader used for early stopping\n",
        "        save_file_name (str ending in '.pt'): file path to save the model state dict\n",
        "        max_epochs_stop (int): maximum number of epochs with no improvement in validation loss for early stopping\n",
        "        n_epochs (int): maximum number of training epochs\n",
        "        print_every (int): frequency of epochs to print training stats\n",
        "\n",
        "    Returns\n",
        "    --------\n",
        "        model (PyTorch model): trained cnn with best weights\n",
        "        history (DataFrame): history of train and validation loss and accuracy\n",
        "    \"\"\"\n",
        "\n",
        "    \n",
        "    # Early stopping intialization\n",
        "    epochs_no_improve = 0\n",
        "    valid_loss_min = np.Inf\n",
        "\n",
        "    valid_max_acc = 0\n",
        "    history = []\n",
        "\n",
        "    # Number of epochs already trained (if using loaded in model weights)\n",
        "    try:\n",
        "        print(f'Model has been trained for: {model.epochs} epochs.\\n')\n",
        "    except:\n",
        "        model.epochs = 0\n",
        "        print(f'Starting Training from Scratch.\\n')\n",
        "\n",
        "    overall_start = timer()\n",
        "\n",
        "    # Main loop\n",
        "    for epoch in range(n_epochs):\n",
        "\n",
        "        # keep track of training and validation loss each epoch\n",
        "        train_loss = 0.0\n",
        "        valid_loss = 0.0\n",
        "\n",
        "        train_acc = 0\n",
        "        valid_acc = 0\n",
        "\n",
        "        # Set to training\n",
        "        model.train()\n",
        "        start = timer()\n",
        "\n",
        "        # Training loop\n",
        "        for ii, (data, target) in enumerate(train_loader):\n",
        "            # Tensors to gpu\n",
        "            if train_on_gpu:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            #print(ii)\n",
        "            # Clear gradients\n",
        "            optimizer.zero_grad()\n",
        "            # Predicted outputs are log probabilities\n",
        "            output = model(data)\n",
        "\n",
        "            # Loss and backpropagation of gradients\n",
        "            loss = criterion(output, target)\n",
        "            loss.backward()\n",
        "            \n",
        "            # Update the parameters\n",
        "            optimizer.step()\n",
        "\n",
        "            # Track train loss by multiplying average loss by number of examples in batch\n",
        "            train_loss += loss.item() * data.size(0)\n",
        "\n",
        "            # Calculate accuracy by finding max log probability\n",
        "            _, pred = torch.max(output, dim=1)\n",
        "            correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "            # Need to convert correct tensor from int to float to average\n",
        "            accuracy = torch.mean(correct_tensor.type(torch.FloatTensor))\n",
        "            # Multiply average accuracy times the number of examples in batch\n",
        "            train_acc += accuracy.item() * data.size(0)\n",
        "\n",
        "            # Track training progress\n",
        "            #print(f'Epoch: {epoch}\\t{100 * (ii + 1) / len(train_loader):.2f}% complete. {timer() - start:.2f} seconds elapsed in epoch.')\n",
        "                \n",
        "                \n",
        "\n",
        "        # After training loops ends, start validation\n",
        "        else:\n",
        "            model.epochs += 1\n",
        "\n",
        "            # Don't need to keep track of gradients\n",
        "            with torch.no_grad():\n",
        "                # Set to evaluation mode\n",
        "                model.eval()\n",
        "\n",
        "                # Validation loop\n",
        "                for data, target in valid_loader:\n",
        "                    # Tensors to gpu\n",
        "                    if train_on_gpu:\n",
        "                        data, target = data.cuda(), target.cuda()\n",
        "\n",
        "                    # Forward pass\n",
        "                    output = model(data)\n",
        "\n",
        "                    # Validation loss\n",
        "                    loss = criterion(output, target)\n",
        "                    # Multiply average loss times the number of examples in batch\n",
        "                    valid_loss += loss.item() * data.size(0)\n",
        "\n",
        "                    # Calculate validation accuracy\n",
        "                    _, pred = torch.max(output, dim=1)\n",
        "                    correct_tensor = pred.eq(target.data.view_as(pred))\n",
        "                    accuracy = torch.mean(\n",
        "                        correct_tensor.type(torch.FloatTensor))\n",
        "                    # Multiply average accuracy times the number of examples\n",
        "                    valid_acc += accuracy.item() * data.size(0)\n",
        "\n",
        "                # Calculate average losses\n",
        "                train_loss = train_loss / len(train_loader.dataset)\n",
        "                valid_loss = valid_loss / len(valid_loader.dataset)\n",
        "\n",
        "                # Calculate average accuracy\n",
        "                train_acc = train_acc / len(train_loader.dataset)\n",
        "                valid_acc = valid_acc / len(valid_loader.dataset)\n",
        "\n",
        "                history.append([train_loss, valid_loss, train_acc, valid_acc])\n",
        "\n",
        "                # Print training and validation results\n",
        "                \n",
        "                print(\n",
        "                    f'\\nEpoch: {epoch} \\tTraining Loss: {train_loss:.4f} \\tValidation Loss: {valid_loss:.4f}'\n",
        "                )\n",
        "                print(\n",
        "                    f'\\t\\tTraining Accuracy: {100 * train_acc:.2f}%\\t Validation Accuracy: {100 * valid_acc:.2f}%'\n",
        "                )\n",
        "\n",
        "                # Save the model if validation loss decreases\n",
        "                if valid_loss < valid_loss_min:\n",
        "                    # Save model\n",
        "                    torch.save(model.state_dict(), save_file_name)\n",
        "                    # Track improvement\n",
        "                    epochs_no_improve = 0\n",
        "                    valid_loss_min = valid_loss\n",
        "                    valid_best_acc = valid_acc\n",
        "                    best_epoch = epoch\n",
        "\n",
        "                # Otherwise increment count of epochs with no improvement\n",
        "                else:\n",
        "                    epochs_no_improve += 1\n",
        "                    # Trigger early stopping\n",
        "                    if epochs_no_improve >= max_epochs_stop:\n",
        "                        print(\n",
        "                            f'\\nEarly Stopping! Total epochs: {epoch}. Best epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n",
        "                        )\n",
        "                        total_time = timer() - overall_start\n",
        "                        print(\n",
        "                            f'{total_time:.2f} total seconds elapsed. {total_time / (epoch+1):.2f} seconds per epoch.'\n",
        "                        )\n",
        "\n",
        "                        # Load the best state dict\n",
        "                        model.load_state_dict(torch.load(save_file_name))\n",
        "                        # Attach the optimizer\n",
        "                        model.optimizer = optimizer\n",
        "\n",
        "                        # Format history\n",
        "                        history = pd.DataFrame(\n",
        "                            history,\n",
        "                            columns=[\n",
        "                                'train_loss', 'valid_loss', 'train_acc',\n",
        "                                'valid_acc'\n",
        "                            ])\n",
        "                        return model, history\n",
        "\n",
        "    # Attach the optimizer\n",
        "    model.optimizer = optimizer\n",
        "    # Record overall time and print out stats\n",
        "    total_time = timer() - overall_start\n",
        "    print(\n",
        "        f'\\nBest epoch: {best_epoch} with loss: {valid_loss_min:.2f} and acc: {100 * valid_acc:.2f}%'\n",
        "    )\n",
        "    print(\n",
        "        f'{total_time:.2f} total seconds elapsed. {total_time / (epoch):.2f} seconds per epoch.'\n",
        "    )\n",
        "    # Format history\n",
        "    history = pd.DataFrame(\n",
        "        history,\n",
        "        columns=['train_loss', 'valid_loss', 'train_acc', 'valid_acc'])\n",
        "    return model, history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_xV8qXiHaKE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_image(image_path):\n",
        "    \"\"\"Process an image path into a PyTorch tensor\"\"\"\n",
        "\n",
        "    image = Image.open(image_path)\n",
        "    # Resize\n",
        "    img = image.resize((256, 256))\n",
        "\n",
        "    # Center crop\n",
        "    width = 256\n",
        "    height = 256\n",
        "    new_width = 224\n",
        "    new_height = 224\n",
        "\n",
        "    left = (width - new_width) / 2\n",
        "    top = (height - new_height) / 2\n",
        "    right = (width + new_width) / 2\n",
        "    bottom = (height + new_height) / 2\n",
        "    img = img.crop((left, top, right, bottom))\n",
        "\n",
        "    # Convert to numpy, transpose color dimension and normalize\n",
        "    img = np.array(img).transpose((2, 0, 1)) / 256\n",
        "\n",
        "    # Standardization\n",
        "    means = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))\n",
        "    stds = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))\n",
        "\n",
        "    img = img - means\n",
        "    img = img / stds\n",
        "\n",
        "    img_tensor = torch.Tensor(img)\n",
        "\n",
        "    return img_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlc7LNU2HcEQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(image_path, model, topk=2):\n",
        "    \"\"\"Make a prediction for an image using a trained model\n",
        "\n",
        "    Params\n",
        "    --------\n",
        "        image_path (str): filename of the image\n",
        "        model (PyTorch model): trained model for inference\n",
        "        topk (int): number of top predictions to return\n",
        "\n",
        "    Returns\n",
        "\n",
        "    \"\"\"\n",
        "    real_class = image_path.split('/')[-2]\n",
        "\n",
        "    # Convert to pytorch tensor\n",
        "    img_tensor = process_image(image_path)\n",
        "\n",
        "    # Resize\n",
        "    if train_on_gpu:\n",
        "        img_tensor = img_tensor.view(1, 3, 224, 224).cuda()\n",
        "    else:\n",
        "        img_tensor = img_tensor.view(1, 3, 224, 224)\n",
        "\n",
        "    # Set to evaluation\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        # Model outputs log probabilities\n",
        "        out = model(img_tensor)\n",
        "        ps = torch.exp(out)\n",
        "\n",
        "        # Find the topk predictions\n",
        "        topk, topclass = ps.topk(topk, dim=1)\n",
        "\n",
        "        # Extract the actual classes and probabilities\n",
        "        top_classes = [\n",
        "            model.idx_to_class[class_] for class_ in topclass.cpu().numpy()[0]\n",
        "        ]\n",
        "        top_p = topk.cpu().numpy()[0]\n",
        "\n",
        "        return img_tensor.cpu().squeeze(), top_p, top_classes, real_class"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zpYivZpHefi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(output, target, topk=(1, )):\n",
        "    \"\"\"Compute the topk accuracy(s)\"\"\"\n",
        "    if train_on_gpu:\n",
        "        output = output.to('cuda')\n",
        "        target = target.to('cuda')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        maxk = max(topk)\n",
        "        batch_size = target.size(0)\n",
        "\n",
        "        # Find the predicted classes and transpose\n",
        "        _, pred = output.topk(k=maxk, dim=1, largest=True, sorted=True)\n",
        "        pred = pred.t()\n",
        "\n",
        "        # Determine predictions equal to the targets\n",
        "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "        res = []\n",
        "\n",
        "        # For each k, find the percentage of correct\n",
        "        for k in topk:\n",
        "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
        "            res.append(correct_k.mul_(100.0 / batch_size).item())\n",
        "        return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwycwxcQIhid",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_pretrained_googlenet(model_name):\n",
        "    \"\"\"Retrieve a pre-trained model from torchvision\n",
        "\n",
        "    Params\n",
        "    -------\n",
        "        model_name (str): name of the model (currently only accepts vgg16 and resnet50)\n",
        "\n",
        "    Return\n",
        "    --------\n",
        "        model (PyTorch model): cnn\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    if model_name == 'googlenet_1':\n",
        "        model = models.googlenet(pretrained=True)\n",
        "        \n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False\n",
        "        \n",
        "        model.fc = nn.Sequential(\n",
        "            nn.Linear(1024,5),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "    \n",
        "    # Move to gpu and parallelize\n",
        "    if train_on_gpu:\n",
        "        model = model.to('cuda')\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKG3arcWIOUh",
        "colab_type": "code",
        "outputId": "d666593a-48ed-4e60-8e2f-f93f74e424f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model1 = get_pretrained_googlenet('googlenet_1')\n",
        "criterion = nn.NLLLoss()\n",
        "optimizer = optim.Adam(model1.parameters())\n",
        "save_file_name = 'googlenet-handgestures'\n",
        "#lets check the trainable parameters\n",
        "for p in optimizer.param_groups[0]['params']:\n",
        "    if p.requires_grad:\n",
        "        print(p.shape)\n",
        "       "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5, 1024])\n",
            "torch.Size([5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjgXH7ytJRhw",
        "colab_type": "code",
        "outputId": "7b37eb5d-b320-48ef-afec-670ec7ba1da6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "model1.class_to_idx = data['train'].class_to_idx\n",
        "model1.idx_to_class = {\n",
        "    idx: class_\n",
        "    for class_, idx in model1.class_to_idx.items()\n",
        "}\n",
        "\n",
        "list(model1.idx_to_class.items())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 'L'), (1, 'fantastic'), (2, 'fist'), (3, 'palm'), (4, 'victory')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZNn685nJTo1",
        "colab_type": "code",
        "outputId": "49067ced-66cc-4ef7-d595-f70dde626578",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model1_trained, history = train(\n",
        "    model1,\n",
        "    criterion,\n",
        "    optimizer,\n",
        "    dataloaders['train'],\n",
        "    dataloaders['val'],\n",
        "    save_file_name=save_file_name,\n",
        "    max_epochs_stop=7,\n",
        "    n_epochs=60)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Starting Training from Scratch.\n",
            "\n",
            "\n",
            "Epoch: 0 \tTraining Loss: 1.4354 \tValidation Loss: 1.3241\n",
            "\t\tTraining Accuracy: 45.12%\t Validation Accuracy: 40.47%\n",
            "\n",
            "Epoch: 1 \tTraining Loss: 1.1143 \tValidation Loss: 1.0732\n",
            "\t\tTraining Accuracy: 67.45%\t Validation Accuracy: 63.91%\n",
            "\n",
            "Epoch: 2 \tTraining Loss: 0.9001 \tValidation Loss: 0.8980\n",
            "\t\tTraining Accuracy: 79.66%\t Validation Accuracy: 78.08%\n",
            "\n",
            "Epoch: 3 \tTraining Loss: 0.7737 \tValidation Loss: 0.8045\n",
            "\t\tTraining Accuracy: 82.47%\t Validation Accuracy: 78.41%\n",
            "\n",
            "Epoch: 4 \tTraining Loss: 0.7064 \tValidation Loss: 0.7342\n",
            "\t\tTraining Accuracy: 83.69%\t Validation Accuracy: 79.93%\n",
            "\n",
            "Epoch: 5 \tTraining Loss: 0.6318 \tValidation Loss: 0.6747\n",
            "\t\tTraining Accuracy: 83.87%\t Validation Accuracy: 82.12%\n",
            "\n",
            "Epoch: 6 \tTraining Loss: 0.5829 \tValidation Loss: 0.6414\n",
            "\t\tTraining Accuracy: 86.03%\t Validation Accuracy: 83.14%\n",
            "\n",
            "Epoch: 7 \tTraining Loss: 0.5232 \tValidation Loss: 0.6053\n",
            "\t\tTraining Accuracy: 87.84%\t Validation Accuracy: 82.97%\n",
            "\n",
            "Epoch: 8 \tTraining Loss: 0.4917 \tValidation Loss: 0.5686\n",
            "\t\tTraining Accuracy: 88.19%\t Validation Accuracy: 84.49%\n",
            "\n",
            "Epoch: 9 \tTraining Loss: 0.4826 \tValidation Loss: 0.5516\n",
            "\t\tTraining Accuracy: 89.01%\t Validation Accuracy: 84.15%\n",
            "\n",
            "Epoch: 10 \tTraining Loss: 0.4767 \tValidation Loss: 0.5131\n",
            "\t\tTraining Accuracy: 86.85%\t Validation Accuracy: 85.83%\n",
            "\n",
            "Epoch: 11 \tTraining Loss: 0.4596 \tValidation Loss: 0.4872\n",
            "\t\tTraining Accuracy: 87.32%\t Validation Accuracy: 87.02%\n",
            "\n",
            "Epoch: 12 \tTraining Loss: 0.4342 \tValidation Loss: 0.4708\n",
            "\t\tTraining Accuracy: 88.37%\t Validation Accuracy: 87.02%\n",
            "\n",
            "Epoch: 13 \tTraining Loss: 0.4133 \tValidation Loss: 0.4866\n",
            "\t\tTraining Accuracy: 89.13%\t Validation Accuracy: 87.02%\n",
            "\n",
            "Epoch: 14 \tTraining Loss: 0.3906 \tValidation Loss: 0.4735\n",
            "\t\tTraining Accuracy: 89.19%\t Validation Accuracy: 85.50%\n",
            "\n",
            "Epoch: 15 \tTraining Loss: 0.4050 \tValidation Loss: 0.4603\n",
            "\t\tTraining Accuracy: 88.49%\t Validation Accuracy: 86.51%\n",
            "\n",
            "Epoch: 16 \tTraining Loss: 0.3864 \tValidation Loss: 0.4276\n",
            "\t\tTraining Accuracy: 88.78%\t Validation Accuracy: 87.86%\n",
            "\n",
            "Epoch: 17 \tTraining Loss: 0.3699 \tValidation Loss: 0.4187\n",
            "\t\tTraining Accuracy: 89.36%\t Validation Accuracy: 87.52%\n",
            "\n",
            "Epoch: 18 \tTraining Loss: 0.3761 \tValidation Loss: 0.4364\n",
            "\t\tTraining Accuracy: 89.25%\t Validation Accuracy: 86.17%\n",
            "\n",
            "Epoch: 19 \tTraining Loss: 0.3396 \tValidation Loss: 0.4194\n",
            "\t\tTraining Accuracy: 90.82%\t Validation Accuracy: 87.02%\n",
            "\n",
            "Epoch: 20 \tTraining Loss: 0.3412 \tValidation Loss: 0.3898\n",
            "\t\tTraining Accuracy: 90.12%\t Validation Accuracy: 87.86%\n",
            "\n",
            "Epoch: 21 \tTraining Loss: 0.3385 \tValidation Loss: 0.3856\n",
            "\t\tTraining Accuracy: 90.59%\t Validation Accuracy: 88.20%\n",
            "\n",
            "Epoch: 22 \tTraining Loss: 0.3468 \tValidation Loss: 0.3739\n",
            "\t\tTraining Accuracy: 89.36%\t Validation Accuracy: 88.36%\n",
            "\n",
            "Epoch: 23 \tTraining Loss: 0.3443 \tValidation Loss: 0.3855\n",
            "\t\tTraining Accuracy: 89.07%\t Validation Accuracy: 86.17%\n",
            "\n",
            "Epoch: 24 \tTraining Loss: 0.3227 \tValidation Loss: 0.3806\n",
            "\t\tTraining Accuracy: 90.47%\t Validation Accuracy: 88.36%\n",
            "\n",
            "Epoch: 25 \tTraining Loss: 0.3119 \tValidation Loss: 0.3584\n",
            "\t\tTraining Accuracy: 91.23%\t Validation Accuracy: 88.87%\n",
            "\n",
            "Epoch: 26 \tTraining Loss: 0.3350 \tValidation Loss: 0.3715\n",
            "\t\tTraining Accuracy: 89.60%\t Validation Accuracy: 87.52%\n",
            "\n",
            "Epoch: 27 \tTraining Loss: 0.3046 \tValidation Loss: 0.3547\n",
            "\t\tTraining Accuracy: 91.06%\t Validation Accuracy: 89.38%\n",
            "\n",
            "Epoch: 28 \tTraining Loss: 0.3220 \tValidation Loss: 0.3817\n",
            "\t\tTraining Accuracy: 90.71%\t Validation Accuracy: 87.02%\n",
            "\n",
            "Epoch: 29 \tTraining Loss: 0.3113 \tValidation Loss: 0.3489\n",
            "\t\tTraining Accuracy: 90.59%\t Validation Accuracy: 89.04%\n",
            "\n",
            "Epoch: 30 \tTraining Loss: 0.2972 \tValidation Loss: 0.3462\n",
            "\t\tTraining Accuracy: 91.41%\t Validation Accuracy: 88.20%\n",
            "\n",
            "Epoch: 31 \tTraining Loss: 0.3013 \tValidation Loss: 0.3325\n",
            "\t\tTraining Accuracy: 90.77%\t Validation Accuracy: 89.04%\n",
            "\n",
            "Epoch: 32 \tTraining Loss: 0.3027 \tValidation Loss: 0.3433\n",
            "\t\tTraining Accuracy: 91.29%\t Validation Accuracy: 89.21%\n",
            "\n",
            "Epoch: 33 \tTraining Loss: 0.2804 \tValidation Loss: 0.3317\n",
            "\t\tTraining Accuracy: 91.70%\t Validation Accuracy: 88.53%\n",
            "\n",
            "Epoch: 34 \tTraining Loss: 0.2966 \tValidation Loss: 0.3345\n",
            "\t\tTraining Accuracy: 90.53%\t Validation Accuracy: 89.54%\n",
            "\n",
            "Epoch: 35 \tTraining Loss: 0.2793 \tValidation Loss: 0.3274\n",
            "\t\tTraining Accuracy: 91.70%\t Validation Accuracy: 89.04%\n",
            "\n",
            "Epoch: 36 \tTraining Loss: 0.3028 \tValidation Loss: 0.3252\n",
            "\t\tTraining Accuracy: 90.01%\t Validation Accuracy: 89.21%\n",
            "\n",
            "Epoch: 37 \tTraining Loss: 0.3085 \tValidation Loss: 0.3309\n",
            "\t\tTraining Accuracy: 90.41%\t Validation Accuracy: 89.38%\n",
            "\n",
            "Epoch: 38 \tTraining Loss: 0.2895 \tValidation Loss: 0.3258\n",
            "\t\tTraining Accuracy: 90.30%\t Validation Accuracy: 89.21%\n",
            "\n",
            "Epoch: 39 \tTraining Loss: 0.2850 \tValidation Loss: 0.3233\n",
            "\t\tTraining Accuracy: 91.82%\t Validation Accuracy: 88.87%\n",
            "\n",
            "Epoch: 40 \tTraining Loss: 0.2891 \tValidation Loss: 0.3285\n",
            "\t\tTraining Accuracy: 90.59%\t Validation Accuracy: 88.53%\n",
            "\n",
            "Epoch: 41 \tTraining Loss: 0.2695 \tValidation Loss: 0.3334\n",
            "\t\tTraining Accuracy: 92.05%\t Validation Accuracy: 88.53%\n",
            "\n",
            "Epoch: 42 \tTraining Loss: 0.2621 \tValidation Loss: 0.3295\n",
            "\t\tTraining Accuracy: 91.93%\t Validation Accuracy: 88.20%\n",
            "\n",
            "Epoch: 43 \tTraining Loss: 0.2665 \tValidation Loss: 0.3030\n",
            "\t\tTraining Accuracy: 92.05%\t Validation Accuracy: 89.71%\n",
            "\n",
            "Epoch: 44 \tTraining Loss: 0.2895 \tValidation Loss: 0.3153\n",
            "\t\tTraining Accuracy: 91.06%\t Validation Accuracy: 88.70%\n",
            "\n",
            "Epoch: 45 \tTraining Loss: 0.2664 \tValidation Loss: 0.3124\n",
            "\t\tTraining Accuracy: 92.23%\t Validation Accuracy: 88.53%\n",
            "\n",
            "Epoch: 46 \tTraining Loss: 0.2589 \tValidation Loss: 0.3031\n",
            "\t\tTraining Accuracy: 91.58%\t Validation Accuracy: 88.87%\n",
            "\n",
            "Epoch: 47 \tTraining Loss: 0.2593 \tValidation Loss: 0.3081\n",
            "\t\tTraining Accuracy: 91.12%\t Validation Accuracy: 89.04%\n",
            "\n",
            "Epoch: 48 \tTraining Loss: 0.2745 \tValidation Loss: 0.2961\n",
            "\t\tTraining Accuracy: 91.23%\t Validation Accuracy: 89.54%\n",
            "\n",
            "Epoch: 49 \tTraining Loss: 0.2667 \tValidation Loss: 0.2809\n",
            "\t\tTraining Accuracy: 91.58%\t Validation Accuracy: 90.73%\n",
            "\n",
            "Epoch: 50 \tTraining Loss: 0.2559 \tValidation Loss: 0.3174\n",
            "\t\tTraining Accuracy: 91.53%\t Validation Accuracy: 88.70%\n",
            "\n",
            "Epoch: 51 \tTraining Loss: 0.2569 \tValidation Loss: 0.3070\n",
            "\t\tTraining Accuracy: 90.77%\t Validation Accuracy: 88.87%\n",
            "\n",
            "Epoch: 52 \tTraining Loss: 0.2541 \tValidation Loss: 0.2993\n",
            "\t\tTraining Accuracy: 91.53%\t Validation Accuracy: 89.38%\n",
            "\n",
            "Epoch: 53 \tTraining Loss: 0.2651 \tValidation Loss: 0.2878\n",
            "\t\tTraining Accuracy: 92.23%\t Validation Accuracy: 90.05%\n",
            "\n",
            "Epoch: 54 \tTraining Loss: 0.2525 \tValidation Loss: 0.2960\n",
            "\t\tTraining Accuracy: 91.35%\t Validation Accuracy: 89.21%\n",
            "\n",
            "Epoch: 55 \tTraining Loss: 0.2697 \tValidation Loss: 0.3152\n",
            "\t\tTraining Accuracy: 90.65%\t Validation Accuracy: 88.03%\n",
            "\n",
            "Epoch: 56 \tTraining Loss: 0.2624 \tValidation Loss: 0.3004\n",
            "\t\tTraining Accuracy: 91.35%\t Validation Accuracy: 88.20%\n",
            "\n",
            "Early Stopping! Total epochs: 56. Best epoch: 49 with loss: 0.28 and acc: 88.20%\n",
            "2271.25 total seconds elapsed. 39.85 seconds per epoch.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlZk_asiLNDy",
        "colab_type": "code",
        "outputId": "2cb4d81f-0260-4171-e160-f30261a7f851",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "for c in ['train_loss', 'valid_loss']:\n",
        "    plt.plot(\n",
        "        history[c], label=c)\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Average Negative Log Likelihood')\n",
        "plt.title('Training and Validation Losses')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a827dab32351>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'valid_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     plt.plot(\n\u001b[1;32m      4\u001b[0m         history[c], label=c)\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KdFK21ZJZEt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "for c in ['train_acc', 'valid_acc']:\n",
        "    plt.plot(\n",
        "        100 * history[c], label=c)\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Average Accuracy')\n",
        "plt.title('Training and Validation Accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dfDnF0VLSF1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_path = test_dir + '/victory/image_101.jpg'\n",
        "img, top_p, top_classes, real_class = predict(img_path, model1_trained)\n",
        "_ = imshow_tensor(img)\n",
        "print(top_p, top_classes, real_class)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h6PjZC8jLUUq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a=[]\n",
        "b=[]\n",
        "for i in range(10):\n",
        "    testiter = iter(dataloaders['test'])\n",
        "    # Get a batch of testing images and labels\n",
        "    features, targets = next(testiter)\n",
        "    if train_on_gpu:\n",
        "        x = accuracy(model1_trained(features.to('cuda')), targets, topk=(1, 2))[0]\n",
        "        y = accuracy(model1_trained(features.to('cuda')), targets, topk=(1, 2))[1]\n",
        "        print(x,y)\n",
        "        a.append(x)\n",
        "        b.append(y)\n",
        "    else:\n",
        "        print(accuracy(model1_trained(features), targets, topk=(1, 2)))\n",
        "        a.append(accuracy(model1_trained(features.to('cuda')), targets, topk=(1, 2))[0])\n",
        "        b.append(accuracy(model1_trained(features.to('cuda')), targets, topk=(1, 2))[1])\n",
        "print(sum(a)/10)\n",
        "print(sum(b)/10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9D0gi9iLW9q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_path = '/content/gdrive/My Drive/fist.jpg'\n",
        "img, top_p, top_classes, real_class = predict(img_path, model1_trained)\n",
        "_ = imshow_tensor(img)\n",
        "print(top_p, top_classes, real_class)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}